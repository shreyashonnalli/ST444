{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary work\n",
    "\n",
    "- I found one of the datasets from the paper\n",
    "- I chose 1 of the features which seemed most appropriate for a polynomial fit for linear regression (basically exactly what the paper did)\n",
    "- Below is some preliminary work where I conduct standard linear regression and then polynomial linear regression using existing package sklearn\n",
    "- I show MSE for both fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('dataset1/dataset1.xlsx')\n",
    "print(dataframe.shape)\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe['V'].sort_values()\n",
    "X = (X-X.mean()) / X.std()\n",
    "y = dataframe['AT'][X.index].values\n",
    "y = (y-y.mean()) / y.std()\n",
    "\n",
    "X = np.reshape(X.values, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"Exhaust Vacuum\")\n",
    "plt.ylabel(\"Average Temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_hat_sklearn = lr.predict(X)\n",
    "\n",
    "pr = PolynomialFeatures(degree=3)\n",
    "X_poly = pr.fit_transform(X)\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_poly, y)\n",
    "y_hat_poly_sklearn = lr_poly.predict(X_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly_sklearn, color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_sklearn = lr.predict(X)\n",
    "y_hat_poly_sklearn = lr_poly.predict(X_poly)\n",
    "\n",
    "print(\"mean squared error for standard linear:\", mean_squared_error(y, y_hat_sklearn))\n",
    "print(\"mean squared error for linear polynomial:\", mean_squared_error(y, y_hat_poly_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going from built-in package to implementing it ourselves\n",
    "\n",
    "- Now using the dataset, I will conduct linear regression, but this time using matrix multiplication and numpy.\n",
    "- I will implement a closed-form based based algorithm before moving onto a gradient descent based algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts an (N * 1) matrix into a (N * h) matrix where h is the number of basis functions ()\n",
    "The degree of the polynomial is (h-1)\n",
    "'''\n",
    "def polynomial_basis_function_transformation(X, h):\n",
    "    powers = np.arange(h)\n",
    "    X_poly = np.power(X, powers)\n",
    "    return X_poly\n",
    "\n",
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "Returns the predictions only\n",
    "'''\n",
    "def lin_reg_poly_closed_form(X, y, h):\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    beta_hat_poly = np.linalg.pinv(X_poly.T @ X_poly) @ X_poly.T @ y\n",
    "    y_hat_poly = X_poly @ beta_hat_poly\n",
    "    print(beta_hat_poly)\n",
    "    return y_hat_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_poly = lin_reg_poly_closed_form(X, y, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly, color = 'green')\n",
    "plt.show()\n",
    "\n",
    "print(\"mean squared error for linear polynomial through numpy:\", mean_squared_error(y, y_hat_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Batch Gradient Descent algorithm for linear regression\n",
    "\n",
    "- We have conducted the closed form solution for polynomial linear regression above ourselves, moving away from sklearn as a package\n",
    "- We now look to implement an iterative algorithm, useful when closed form solution is computationally prohibitive, such as when $X^TX$ is $10,000*10,000$ leading to matrix inversion times being extremely long\n",
    "- We will initially implement Batch Gradient Descent and parallelize it before finally moving onto Stochastic Gradient Descent, and then parallelizing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Parallelized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "But this time linear regression is conducted through iterative gradient descent\n",
    "MSE as you iterate through the algorithm is shown\n",
    "Returns the predictions only\n",
    "'''\n",
    "def lin_reg_poly_batch_gradient_descent(X, y, h, alpha, n):\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    beta_hat_poly = np.random.rand(h)\n",
    "    for i in range(n):\n",
    "        y_hat_poly = X_poly @ beta_hat_poly\n",
    "        beta_hat_poly = beta_hat_poly - alpha * (X_poly.T @ (y_hat_poly - y))\n",
    "        print(\"MSE in iteration\", i, \": \", mean_squared_error(y, y_hat_poly))\n",
    "    return y_hat_poly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_poly_bgd = lin_reg_poly_batch_gradient_descent(X, y, 4, 0.00001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly_bgd, color = 'green')\n",
    "plt.show()\n",
    "\n",
    "print(\"mean squared error for linear polynomial through numpy via gradient descent:\", mean_squared_error(y, y_hat_poly_bgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized version\n",
    "- We now implement the parallelized version of Batch Gradient Descent\n",
    "- We can expect to see clear advantages to the Batch Gradient Descent algorithm when using parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
