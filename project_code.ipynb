{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary work\n",
    "\n",
    "- I found one of the datasets from the paper\n",
    "- I chose 1 of the features which seemed most appropriate for a polynomial fit for linear regression (basically exactly what the paper did)\n",
    "- Below is some preliminary work where I conduct standard linear regression and then polynomial linear regression using existing package sklearn\n",
    "- I show MSE for both fits\n",
    "\n",
    "\n",
    "- We will be doing linear regression in this notebook: $y = X\\beta + \\epsilon$\n",
    "- We will be choosing only a single regressor to predict the dependent variable\n",
    "- We have chosen a dataset which requires polynomial linear regression\n",
    "- As a result the line we're trying to fit is $y=\\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\epsilon$\n",
    "- Need to transform (N*1) matrix $X$ to (N*4) matrix as a result where ith row is $\\begin{bmatrix} 1 & x_i & x_i^2 & x_i^3 \\end{bmatrix}$\n",
    "- After this transformation, initially we will use the closed form solution for the Least Squares Estimator to fit the data: $\\hat{\\beta} = (X^T X)^{-1}X^T y$\n",
    "    - Here $\\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\hat{\\beta_2} \\\\ \\hat{\\beta_3} \\end{bmatrix}$\n",
    "- Then we move onto an iterative gradient descent approach for LSE Estimation of Linear Regression. SGD is one of the algorithms here!\n",
    "    - Batch Gradient Descent where we use the entire dataset: $\\hat{\\beta}_{k+1} = \\hat{\\beta}_{k} - \\alpha X^T(\\hat{y} - y)$\n",
    "        - $\\hat{\\beta}_{k} = \\begin{bmatrix} \\hat{\\beta_0}_{k} \\\\ \\hat{\\beta_1}_{k} \\\\ \\hat{\\beta_2}_{k} \\\\ \\hat{\\beta_3}_{k} \\end{bmatrix}, y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, X = \\begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 \\\\ 1 & x_2 & x_2^2 & x_2^3 \\\\ \\vdots & \\ddots \\\\ 1 & x_n & x_n^2 & x_n^3 \\end{bmatrix}$\n",
    "    - Stochastic Gradient Descent where we use just a single randomly selected sample: $\\hat{\\beta}_{k+1} = \\hat{\\beta}_{k} - \\alpha(\\hat{y_i} - y) x_i^T$\n",
    "        - Now the $y, y_i$ are both scalar, and $x_i$ is not a matrix but rather a vector of a single randomly selected row from $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from multiprocessing import Process, Pool\n",
    "import time\n",
    "import os\n",
    "import workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('dataset1/dataset1.xlsx')\n",
    "print(dataframe.shape)\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe['V'].sort_values()\n",
    "X = (X-X.mean()) / X.std()\n",
    "y = dataframe['AT'][X.index].values\n",
    "y = (y-y.mean()) / y.std()\n",
    "\n",
    "X = np.reshape(X.values, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"Exhaust Vacuum\")\n",
    "plt.ylabel(\"Average Temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_hat_sklearn = lr.predict(X)\n",
    "\n",
    "pr = PolynomialFeatures(degree=3)\n",
    "X_poly = pr.fit_transform(X)\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_poly, y)\n",
    "y_hat_poly_sklearn = lr_poly.predict(X_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly_sklearn, color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_sklearn = lr.predict(X)\n",
    "y_hat_poly_sklearn = lr_poly.predict(X_poly)\n",
    "\n",
    "print(\"mean squared error for standard linear:\", mean_squared_error(y, y_hat_sklearn))\n",
    "print(\"mean squared error for linear polynomial:\", mean_squared_error(y, y_hat_poly_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going from built-in package to implementing it ourselves\n",
    "\n",
    "- Now using the dataset, I will conduct linear regression, but this time using matrix multiplication and numpy.\n",
    "- I will implement a closed-form based based algorithm before moving onto a gradient descent based algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts an (N * 1) matrix into a (N * h) matrix where h is the number of basis functions ()\n",
    "The degree of the polynomial is (h-1)\n",
    "'''\n",
    "def polynomial_basis_function_transformation(X, h):\n",
    "    powers = np.arange(h)\n",
    "    X_poly = np.power(X, powers)\n",
    "    return X_poly\n",
    "\n",
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "Returns the predictions only\n",
    "'''\n",
    "def lin_reg_poly_closed_form(X, y, h):\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    beta_hat_poly = np.linalg.pinv(X_poly.T @ X_poly) @ X_poly.T @ y\n",
    "    y_hat_poly = X_poly @ beta_hat_poly\n",
    "    print(beta_hat_poly)\n",
    "    return y_hat_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_poly = lin_reg_poly_closed_form(X, y, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly, color = 'green')\n",
    "plt.show()\n",
    "\n",
    "print(\"mean squared error for linear polynomial through numpy (closed form):\", mean_squared_error(y, y_hat_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Batch Gradient Descent algorithm for linear regression\n",
    "\n",
    "- We have conducted the closed form solution for polynomial linear regression above ourselves, moving away from sklearn as a package\n",
    "- We now look to implement an iterative algorithm, useful when closed form solution is computationally prohibitive, such as when $X^TX$ is $10,000*10,000$ leading to matrix inversion times being extremely long\n",
    "- We will initially implement Batch Gradient Descent and parallelize it before finally moving onto Stochastic Gradient Descent, and then parallelizing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Parallelized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "But this time linear regression is conducted through iterative batch gradient descent\n",
    "MSE as you iterate through the algorithm is shown\n",
    "Returns the predictions only\n",
    "'''\n",
    "def lin_reg_poly_bgd(X, y, h, alpha, n):\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    beta_hat_poly = np.random.rand(h)\n",
    "    for i in range(n):\n",
    "        y_hat_poly = X_poly @ beta_hat_poly\n",
    "        beta_hat_poly = beta_hat_poly - alpha * (X_poly.T @ (y_hat_poly - y))\n",
    "        print(\"MSE in iteration\", i, \": \", mean_squared_error(y, y_hat_poly))\n",
    "    return y_hat_poly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_poly_bgd = lin_reg_poly_bgd(X, y, 4, 0.00001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly_bgd, color = 'green')\n",
    "plt.show()\n",
    "\n",
    "print(\"mean squared error for linear polynomial through numpy (gradient descent):\", mean_squared_error(y, y_hat_poly_bgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized version\n",
    "- We now implement the parallelized version of Batch Gradient Descent\n",
    "- We can expect to see clear advantages to the Batch Gradient Descent algorithm when using parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "- We will now implement the non-parallelized version of SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Parallelized Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "But this time linear regression is conducted through iterative gradient descent\n",
    "Specifically stochastic gradient descent where we just choose a single sample from the the dataset\n",
    "MSE as you iterate through the algorithm is shown\n",
    "Returns the predictions only\n",
    "'''\n",
    "def lin_reg_poly_sgd(X, y, h, alpha, n):\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    beta_hat_poly = np.random.rand(h)\n",
    "    for i in range(n):\n",
    "        idx = np.random.randint(0, X_poly.shape[0])\n",
    "        X_sample = X_poly[idx, :]\n",
    "        y_sample = y[idx]\n",
    "        y_hat_sample_poly = X_sample @ beta_hat_poly\n",
    "        beta_hat_poly = beta_hat_poly - alpha * (X_sample.T * (y_hat_sample_poly - y_sample))\n",
    "        \n",
    "        y_hat_poly = X_poly @ beta_hat_poly\n",
    "        print(\"MSE in iteration\", i, \": \", mean_squared_error(y, y_hat_poly))\n",
    "    return y_hat_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_poly_sgd(X, y, 4, 0.0001, 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized Version 1\n",
    "- Here is a very simple example of how to use multiprocessing\n",
    "- Note that the actual function which the worker processes in parallel HAS TO BE IN ANOTHER FILE\n",
    "- This is why workers.py exists\n",
    "- Try by changing workers.f to just f and f2 from this notebook. You will see what I mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main line\n",
      "module name: workers\n",
      "parent process: 72687\n",
      "process id: 96085\n",
      "\n",
      "hello bob1\n",
      "function f\n",
      "module name: workers\n",
      "parent process: 96085\n",
      "process id: 98395\n",
      "\n",
      "hello bob2\n",
      "function f\n",
      "module name: workers\n",
      "parent process: 96085\n",
      "process id: 98394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def info(title):\n",
    "    print(title)\n",
    "    print('module name:', __name__)\n",
    "    print('parent process:', os.getppid())\n",
    "    print('process id:', os.getpid())\n",
    "    \n",
    "    \n",
    "def f(name):\n",
    "    print(\"hello bob1\")\n",
    "    info('function f')\n",
    "    print()\n",
    "    \n",
    "def f2(name):\n",
    "    time.sleep(5)\n",
    "    print(\"hello bob2\")\n",
    "    info('function f')\n",
    "    print()\n",
    "\n",
    "workers.info('main line')\n",
    "p1 = Process(target=workers.f, args=('bob1',))\n",
    "p2 = Process(target=workers.f2, args=('bob2',))\n",
    "\n",
    "p2.start()\n",
    "p1.start()\n",
    "p2.join()\n",
    "p1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_poly_sgd_parallel(X, y, h, alpha, n, t):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
