{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary work\n",
    "\n",
    "- I found one of the datasets from the paper\n",
    "- I chose 1 of the features which seemed most appropriate for a polynomial fit for linear regression (basically exactly what the paper did)\n",
    "- Below is some preliminary work where I conduct standard linear regression and then polynomial linear regression using existing package sklearn\n",
    "- I show MSE for both fits\n",
    "\n",
    "\n",
    "- We will be doing linear regression in this notebook: $y = X\\beta + \\epsilon$\n",
    "- We will be choosing only a single regressor to predict the dependent variable\n",
    "- We have chosen a dataset which requires polynomial linear regression\n",
    "- As a result the line we're trying to fit is $y=\\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\epsilon$\n",
    "- Need to transform (N * 1) matrix $X$ to (N * 4) matrix as a result where ith row is $\\begin{bmatrix} 1 & x_i & x_i^2 & x_i^3 \\end{bmatrix}$\n",
    "- After this transformation, initially we will use the closed form solution for the Least Squares Estimator to fit the data: $\\hat{\\beta} = (X^T X)^{-1}X^T y$\n",
    "    - Here $\\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\hat{\\beta_2} \\\\ \\hat{\\beta_3} \\end{bmatrix}$\n",
    "- Then we move onto an iterative gradient descent approach for LSE Estimation of Linear Regression. SGD is one of the algorithms here!\n",
    "    - Batch Gradient Descent where we use the entire dataset: $\\hat{\\beta}_{k+1} = \\hat{\\beta}_{k} - \\alpha X^T(\\hat{y} - y)$\n",
    "        - $\\hat{\\beta}_{k} = \\begin{bmatrix} \\hat{\\beta_0}_{k} \\\\ \\hat{\\beta_1}_{k} \\\\ \\hat{\\beta_2}_{k} \\\\ \\hat{\\beta_3}_{k} \\end{bmatrix}, y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, X = \\begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 \\\\ 1 & x_2 & x_2^2 & x_2^3 \\\\ \\vdots & \\ddots \\\\ 1 & x_n & x_n^2 & x_n^3 \\end{bmatrix}$\n",
    "    - Stochastic Gradient Descent where we use just a single randomly selected sample: $\\hat{\\beta}_{k+1} = \\hat{\\beta}_{k} - \\alpha(\\hat{y_i} - y) x_i^T$\n",
    "        - Now the $y, y_i$ are both scalar, and $x_i$ is not a matrix but rather a vector of a single randomly selected row from $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from multiprocessing import Process, Pool\n",
    "import time\n",
    "import workers\n",
    "import SGD_Zinkevich\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('dataset1/dataset1.xlsx')\n",
    "print(dataframe.shape)\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe['V'].sort_values()\n",
    "X = (X-X.mean()) / X.std()\n",
    "y = dataframe['AT'][X.index].values\n",
    "y = (y-y.mean()) / y.std()\n",
    "\n",
    "X = np.reshape(X.values, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"Exhaust Vacuum\")\n",
    "plt.ylabel(\"Average Temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_hat_sklearn = lr.predict(X)\n",
    "\n",
    "pr = PolynomialFeatures(degree=3)\n",
    "X_poly = pr.fit_transform(X)\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_poly, y)\n",
    "y_hat_poly_sklearn = lr_poly.predict(X_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly_sklearn, color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_sklearn = lr.predict(X)\n",
    "y_hat_poly_sklearn = lr_poly.predict(X_poly)\n",
    "\n",
    "print(\"mean squared error for standard linear:\", mean_squared_error(y, y_hat_sklearn))\n",
    "print(\"mean squared error for linear polynomial:\", mean_squared_error(y, y_hat_poly_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going from built-in package to implementing it ourselves\n",
    "\n",
    "- Now using the dataset, I will conduct linear regression, but this time using matrix multiplication and numpy.\n",
    "- I will implement a closed-form based based algorithm before moving onto a gradient descent based algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts an (N * 1) matrix into a (N * h) matrix where h is the number of basis functions ()\n",
    "The degree of the polynomial is (h-1)\n",
    "'''\n",
    "def polynomial_basis_function_transformation(X, h):\n",
    "    powers = np.arange(h)\n",
    "    X_poly = np.power(X, powers)\n",
    "    return X_poly\n",
    "\n",
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "Returns the predictions only\n",
    "'''\n",
    "def lin_reg_poly_closed_form(X, y, h):\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    beta_hat_poly = np.linalg.pinv(X_poly.T @ X_poly) @ X_poly.T @ y\n",
    "    y_hat_poly = X_poly @ beta_hat_poly\n",
    "    return y_hat_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_poly = lin_reg_poly_closed_form(X, y, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly, color = 'green')\n",
    "plt.show()\n",
    "\n",
    "print(\"mean squared error for linear polynomial through numpy (closed form):\", mean_squared_error(y, y_hat_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Batch Gradient Descent algorithm for linear regression\n",
    "\n",
    "- We have implemented the closed form solution for polynomial linear regression ourselves above, moving away from sklearn as a package\n",
    "- We now look to implement an iterative algorithm, useful when closed form solution is computationally prohibitive, such as when $X^TX$ is $10,000*10,000$ leading to matrix inversion times being extremely long (in the above case it is only $4 * 4$)\n",
    "- We will initially implement Batch Gradient Descent and parallelize it before finally moving onto Stochastic Gradient Descent, and then parallelizing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Parallelized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "But this time linear regression is conducted through iterative batch gradient descent\n",
    "MSE as you iterate through the algorithm is shown\n",
    "Returns the predictions only\n",
    "'''\n",
    "def lin_reg_poly_bgd(X, y, h, alpha, n):\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    beta_hat_poly = np.random.rand(h)\n",
    "    for i in range(n):\n",
    "        y_hat_poly = X_poly @ beta_hat_poly\n",
    "        beta_hat_poly = beta_hat_poly - alpha * (X_poly.T @ (y_hat_poly - y))\n",
    "        if i % 100000 == 0:\n",
    "            print(\"MSE in iteration\", i, \": \", mean_squared_error(y, y_hat_poly))\n",
    "    return y_hat_poly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_poly_bgd = lin_reg_poly_bgd(X, y, 4, 0.00001, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = 'blue')\n",
    "plt.plot(X, y_hat_sklearn, color = 'firebrick')\n",
    "plt.plot(X, y_hat_poly_bgd, color = 'green')\n",
    "plt.show()\n",
    "\n",
    "print(\"mean squared error for linear polynomial through numpy (gradient descent):\", mean_squared_error(y, y_hat_poly_bgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized version\n",
    "- We now implement the parallelized version of Batch Gradient Descent\n",
    "- We can expect to see clear advantages to the Batch Gradient Descent algorithm when using parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "- We will now implement the non-parallelized version of SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Parallelized Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "But this time linear regression is conducted through iterative gradient descent\n",
    "Specifically stochastic gradient descent where we just choose a single sample from the the dataset\n",
    "MSE as you iterate through the algorithm is shown\n",
    "Returns the predictions only\n",
    "'''\n",
    "def lin_reg_poly_sgd(X, y, h, alpha, n):\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    beta_hat_poly = np.random.rand(h)\n",
    "    for i in range(n):\n",
    "        idx = np.random.randint(0, X_poly.shape[0])\n",
    "        X_sample = X_poly[idx, :]\n",
    "        y_sample = y[idx]\n",
    "        y_hat_sample_poly = X_sample @ beta_hat_poly\n",
    "        beta_hat_poly = beta_hat_poly - alpha * (X_sample.T * (y_hat_sample_poly - y_sample))\n",
    "        \n",
    "        y_hat_poly = X_poly @ beta_hat_poly\n",
    "        if i % 100000 == 0:\n",
    "            print(\"MSE in iteration\", i, \": \", mean_squared_error(y, y_hat_poly))\n",
    "    return y_hat_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "lin_reg_poly_sgd(X, y, 4, 0.00001, 50000000)\n",
    "diff = datetime.now() - start\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of the Multiprocessing library\n",
    "- Here is a very simple example of how to use multiprocessing\n",
    "- Note that the actual function which the worker processes in parallel HAS TO BE IN ANOTHER FILE\n",
    "- This is why workers.py and SGD_Zinkevich.py exists\n",
    "- Try by changing workers.f to just f and f2 defined in this notebook. You will see what I mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(title):\n",
    "    print(title)\n",
    "    print('module name:', __name__)\n",
    "    print('parent process:', os.getppid())\n",
    "    print('process id:', os.getpid())\n",
    "    \n",
    "    \n",
    "def f(name):\n",
    "    print(\"hello bob1\")\n",
    "    info('function f')\n",
    "    print()\n",
    "    \n",
    "def f2(name):\n",
    "    time.sleep(5)\n",
    "    print(\"hello bob2\")\n",
    "    info('function f')\n",
    "    print()\n",
    "\n",
    "workers.info('main line')\n",
    "p1 = Process(target=workers.f, args=('bob1',))\n",
    "p2 = Process(target=workers.f2, args=('bob2',))\n",
    "\n",
    "p2.start()\n",
    "p1.start()\n",
    "p2.join()\n",
    "p1.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zinkevich et al's attempt at parallelizing SGD\n",
    "- Implementing parallelization of SGD according to the paper \"Parallelized Stochastic Gradient Descent\" published by Zinkevich et al (References in report).\n",
    "- We have extended upon the results achieved by the paper slightly. The procedure is explained below\n",
    "\n",
    "### Preliminary\n",
    "- Please read the descriptions before each function and try understand what each function does\n",
    "    - lin_reg_poly_sgd_parallel implements the parallel SGD algorithm with t processes for n iterations. Returns the MSE after n iterations.\n",
    "    - get_mse_mean_var implements the parallel SGD algorithm with t processes for n iterations an i amount of times and returns the sample mean of MSEs and sample variance of MSEs.\n",
    "    - mse_mean_var_evolution allows us to see the evolution of the average and sample variance of MSEs as you increase the iterations. This provides us snapshots of model performances throughout training for different process counts in the parallel SGD algorithm.\n",
    "\n",
    "### Procedure\n",
    "1. Repeat the below for t = 1, 2, 4, 8 (We conduct the experiment for 1 process, 2 processes, 4 processes, 8 processes in the algorithm).\n",
    "2. Call mse_mean_var_evolution() with the above different values of t. This tells us how the training process evolves when using different number of processes achieving parallelism. Each call to mse_mean_var_evolution returns an array of MSE means and variances, each as you increase n or in other words, as train the model longer.\n",
    "3. We plot the array of MSE means and MSE variances for different process counts on the same plot allowing us to compare.\n",
    "\n",
    "### Results\n",
    "We successfully replicated results achieved by Zinkevich et al. He noticed that theoretically as you increase processor count, you should achieve a reduction in variance for a fixed iteration n. He also noted that you should observe a lower mean MSE as you increase the process count. This is the performance benefit to parallelism - we see better performances earlier on in training as a result of the parallelism. This is because we train t models in parallel, resulting in t models seeing different subsets of the data. In the end when you aggregate all the models, the resulting model will have seen more of the data in the same wall clock time compared to a model with a lower process count. However the benefits of parallelism diminish after you train for an extended amount of time as performances of all models converge to the lowest reference MSE as seen in Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions.\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "But this time linear regression is conducted through an iterative gradient descent algorithm. \n",
    "Specifically stochastic gradient descent where we just choose a single sample from the the dataset.\n",
    "\n",
    "This time t processes conduct SGD in parallel on the entire dataset exactly like the procedure in Zinkevich et al. \n",
    "Each process updates parameters for n iterations.\n",
    "\n",
    "After each thread has returned estimates beta_hats, we aggregate them to get the final beta_hat\n",
    "\n",
    "Returns the MSE only as we are only measuring performance.\n",
    "'''\n",
    "\n",
    "def lin_reg_poly_sgd_parallel(X, y, h, alpha, n, t):\n",
    "    # start = datetime.now()\n",
    "    \n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    \n",
    "    # curr1 = datetime.now()\n",
    "    # diff1 = curr1 - start\n",
    "    \n",
    "    with Pool(processes=t) as p:\n",
    "        \n",
    "        # curr2 = datetime.now()\n",
    "        # diff2 = curr2 - curr1\n",
    "        \n",
    "        outputs = p.starmap(SGD_Zinkevich.lin_reg_poly_sgd, [[X_poly, y, h, alpha, n]] * t)\n",
    "        outputs = np.array(outputs)\n",
    "        \n",
    "        # curr3 = datetime.now()\n",
    "        # diff3 = curr3 - curr2\n",
    "        \n",
    "    beta_hat_poly = np.sum(outputs, axis=0) / t\n",
    "    y_hat_poly = X_poly @ beta_hat_poly\n",
    "    return mean_squared_error(y, y_hat_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function calls the above function but for i amount of times.\n",
    "Lets say we wanted to test the parallelized SGD algorithm after 1000 iterations of the dataset. \n",
    "We want to measure the benefits of parallelism for SGD so rationally we look at the MSE after conducting the parallel SGD algorithm (as described by the paper) with 1 process(equivalent to non parallel SGD), 2 processes, 4 processes, 8 processes, etc.\n",
    "But running the SGD algorithm with t processes for n iterations twice will result in two different MSEs. This is because of the inherent stochastic nature of the algorithm where we randomly sample an observation.\n",
    "Hence to get a more informative measure of the performance of the parallelized SGD algorithm with t processes after n iterations, we need to repeat the experiment i times, and obtain a sample of the MSEs, and then calculate the mean and the variance.\n",
    "Comparing the mean of the MSE samples of two different stochastic models after n iterations makes more sense than comparing single observations of the MSEs.\n",
    "We also compare the variances, and desire such that the variances of the MSE samples reduces as we increase the number of processes t used in training a model. This indicates that parallelism has some benefit.\n",
    "'''\n",
    "\n",
    "def get_mse_mean_var(X, y, h, alpha, n, t, i):\n",
    "    mses = []\n",
    "    for i in range(i):\n",
    "        mse = lin_reg_poly_sgd_parallel(X, y, h, alpha, n, t)\n",
    "        mses.append(mse)\n",
    "    mses = np.array(mses)\n",
    "    return np.mean(mses), np.var(mses)\n",
    "    \n",
    "    \n",
    "'''\n",
    "We have discussed how the above function measures the performance of a parallelized SGD algorithm with t threads after n iterations.\n",
    "We want to see if parallelism has any sort of benefit throughout training, i.e. if we obtain a lower average MSE for the same number of iterations for a model with a higher process count, or maybe a lower variance of MSE for a higher process count if the MSEs are the same after n iterations.\n",
    "Hence the below function implements the above function but gives us snapshots of model performance throughout training. This is allowing us to see the evolution of the performance (through average MSE) for different models throughout training.\n",
    "'''\n",
    "def mse_mean_var_evolution(X, y, h, alpha, n, t, i, means, variances):\n",
    "    x_ = [10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000]\n",
    "    for x in x_:\n",
    "        mean, var = get_mse_mean_var(X, y, h, alpha, x, t, i)\n",
    "        means.append(mean)\n",
    "        variances.append(var)\n",
    "        print(means)\n",
    "        print(variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The below cell took 8 minutes to run on apple M1 Macbook air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means1 = []\n",
    "variances1 = []\n",
    "mse_mean_var_evolution(X, y, 4, 0.0001, 10, 1, 100, means1, variances1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The below cell took 9:15 minutes to run on apple M1 Macbook air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means2 = []\n",
    "variances2 = []\n",
    "mse_mean_var_evolution(X, y, 4, 0.0001, 10, 2, 100, means2, variances2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The below cell took 12:35 minutes to run on apple M1 Macbook air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means4 = []\n",
    "variances4 = []\n",
    "mse_mean_var_evolution(X, y, 4, 0.0001, 10, 4, 100, means4, variances4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The below cell took 23 minutes to run on apple M1 Macbook air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means8 = []\n",
    "variances8 = []\n",
    "mse_mean_var_evolution(X, y, 4, 0.0001, 10, 8, 100, means8, variances8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], means1, label = \"1 Process\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], means2, label = \"2 Processes\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], means4, label = \"4 Processes\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], means8, label = \"8 Processes\")\n",
    "plt.title(\"Evolution of the average MSE throughout training for parallel computing based SGD with different process counts\")\n",
    "plt.ylabel(\"Mean Square Error Estimate\")\n",
    "plt.xlabel(\"Iterations per process\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], variances1, label = \"1 Process Variance\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], variances2, label = \"2 Processes Variance\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], variances4, label = \"4 Processes Variance\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], variances8, label = \"8 Processes Variance\")\n",
    "plt.title(\"Evolution of the variance of the MSE estimates throughout training\")\n",
    "plt.ylabel(\"Variance of Mean Square Error Estimate\")\n",
    "plt.xlabel(\"Iterations per process\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], variances1, label = \"1 Process Variance\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], variances2, label = \"2 Processes Variance\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], variances4, label = \"4 Processes Variance\")\n",
    "plt.plot([10, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 9000], variances8, label = \"8 Processes Variance\")\n",
    "plt.title(\"Evolution of the variance of the MSE estimates throughout training (log scale)\")\n",
    "plt.ylabel(\"Variance of Mean Square Error Estimate\")\n",
    "plt.xlabel(\"Iterations per process\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism with threads instead of processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conducts Linear Regression but initially transforms data using polynomial basis functions\n",
    "Takes in an (N * 1) matrix, converts it into a (N * h) matrix\n",
    "Performs linear regression on the (N*h) matrix resulting in h weights - betas\n",
    "But this time linear regression is conducted through iterative gradient descent\n",
    "Specifically stochastic gradient descent where we just choose a single sample from the the dataset\n",
    "\n",
    "This time t threads conduct sgd in parallel on the entire dataset. \n",
    "Each thread does it for n iterations.\n",
    "\n",
    "After each thread has returned estimates beta_hats, we aggregate them to get the final beta_hat\n",
    "\n",
    "Returns the predictions only\n",
    "'''\n",
    "\n",
    "def lin_reg_poly_sgd_parallel_threads(X, y, h, alpha, n, t):\n",
    "    start = datetime.now()\n",
    "    X_poly = polynomial_basis_function_transformation(X, h)\n",
    "    \n",
    "    curr1 = datetime.now()\n",
    "    diff1 = curr1 - start\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=t) as executor:\n",
    "        curr2 = datetime.now()\n",
    "        diff2 = curr2 - curr1\n",
    "        \n",
    "        futures = [executor.submit(SGD_Zinkevich.lin_reg_poly_sgd, X_poly, y, h, alpha, n) for _ in range(t)]\n",
    "        outputs = []\n",
    "        for future in as_completed(futures):\n",
    "            outputs.append(future.result())\n",
    "        outputs = np.array(outputs)\n",
    "        \n",
    "        curr3 = datetime.now()\n",
    "        diff3 = curr3 - curr2\n",
    "        \n",
    "    beta_hat_poly = np.sum(outputs, axis=0) / t\n",
    "    y_hat_poly = X_poly @ beta_hat_poly\n",
    "    print(\"mean squared error for linear polynomial through SGD (parallel):\", mean_squared_error(y, y_hat_poly))\n",
    "    print(\"Times:\")\n",
    "    print(diff1)\n",
    "    print(diff2)\n",
    "    print(diff3)\n",
    "    return y_hat_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_poly_sgd_parallel_threads(X, y, 4, 0.0000001, 500, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_poly_sgd_parallel_threads(X, y, 4, 0.0000001, 500000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_poly_sgd_parallel_threads(X, y, 4, 0.0000001, 500000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_poly_sgd_parallel_threads(X, y, 4, 0.0000001, 500000, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "- Comment regularly\n",
    "- Branch out in git while doing own work and merge back in\n",
    "- Regularly explain linear algebra/parallelisation algorithm in markdown above cells as he will run code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
